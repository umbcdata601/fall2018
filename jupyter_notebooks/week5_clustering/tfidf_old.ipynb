{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf\n",
    "\n",
    "https://stevenloria.com/tf-idf/ <BR>\n",
    "This post now uses TextBlob for breaking up the text into words and getting the word counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import fnmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes \"term frequency\" which is the number of times a word appears in a document, \n",
    "# normalized by dividing by the total number of words in document. \n",
    "def term_freq(term, list_of_words_in_document):\n",
    "    return list_of_words_in_document.count(term)/(len(list_of_words_in_document)*1.0)\n",
    "\n",
    "# returns the number of documents containing word. \n",
    "def number_of_documents_containing(term,all_documents):\n",
    "    countr=0\n",
    "    for this_doc in all_documents:\n",
    "        if (term in this_doc):\n",
    "            countr+=1\n",
    "    return countr\n",
    "\n",
    "# computes \"inverse document frequency\" which measures \n",
    "# how common a word is among all documents in corpus. \n",
    "# The more common a word is, the lower its idf. \n",
    "# Take the ratio of the total number of documents to the number of documents containing word, \n",
    "# then take the log of that. Add 1 to the divisor to prevent division by zero.\n",
    "def inverse_doc_freq(term, all_documents):\n",
    "    return math.log(len(all_documents) / ( 1.0 + number_of_documents_containing(term, all_documents)))\n",
    "\n",
    "# computes the TF-IDF score. It's the product of tf and idf.\n",
    "def tfidf(term, list_of_words_in_document, all_documents):\n",
    "    return term_freq(term, list_of_words_in_document) * inverse_doc_freq(term, all_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents={}\n",
    "all_words_from_all_docs=[]\n",
    "all_terms=[]\n",
    "foldr='../week6_getting_data/essays'\n",
    "fname='*.dat'\n",
    "for file_name in os.listdir(foldr):\n",
    "    if fnmatch.fnmatch(file_name, fname):\n",
    "        f=open(foldr+'/'+file_name,'r')\n",
    "        #output=f.read().replace(\"\\n\",\" \")\n",
    "        words_in_file=f.read().split(\"\\n\")\n",
    "        f.close()\n",
    "        while \"\" in words_in_file:\n",
    "            words_in_file.remove(\"\")\n",
    "        all_documents[file_name]=words_in_file\n",
    "        for this_word in words_in_file:\n",
    "            all_words_from_all_docs.append(this_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(foldr):\n",
    "    if fnmatch.fnmatch(file_name, fname):\n",
    "        print(foldr+\"/\"+file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample sizes are small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list_in_this_doc = all_documents[list(all_documents.keys())[0]]\n",
    "print(len(word_list_in_this_doc))\n",
    "print(len(all_words_from_all_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_name, word_list_in_this_doc in all_documents.items():\n",
    "    if (len(word_list_in_this_doc)==0):\n",
    "        print(\"error: empty input file\"+doc_name)\n",
    "    else:\n",
    "        dic_of_terms={}\n",
    "        for this_term in word_list_in_this_doc:\n",
    "            dic_of_terms[this_term] = tfidf(this_term, word_list_in_this_doc, all_words_from_all_docs)\n",
    "        #print(dic_of_terms)\n",
    "        print('\\n'+doc_name)\n",
    "        terms_in_doc_sorted_by_score=sorted(dic_of_terms.items(), key=lambda x: x[1], reverse=True)\n",
    "        for this_tup in terms_in_doc_sorted_by_score[0:40]:\n",
    "            print(this_tup)\n",
    "#        for indx in range(10):\n",
    "#            print(terms_in_doc_sorted_by_score[indx][0] + \":\"+str(terms_in_doc_sorted_by_score[indx][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
